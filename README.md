# Common-Adversarial-Attacks
This project tests the vulnerability of a neural network model to adversarial attacks using the Fast Gradient Sign Method (FGSM). The method introduces small, human-imperceptible perturbations to input data, which can lead the model to make incorrect predictions
